{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### report of implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from classes2048 import state, tbl\n",
    "from time import sleep\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functions import mv\n",
    "from pynput.keyboard import Key, Controller\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "keys= [Key.right,Key.down,Key.up,Key.left]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game=state()\n",
    "table=tbl(game)\n",
    "keyboard = Controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.getprops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.gettbl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_input_neurons = len(env.reset())\n",
    "# num_ouptut_neurons = env.action_space.n\n",
    "\n",
    "num_input_neurons = 16\n",
    "num_ouptut_neurons = 4\n",
    "num_hidden_layer_neurons_1 = 256\n",
    "\n",
    "gamma = 0.99\n",
    "max_epsilon = 0.9\n",
    "epsilon = max_epsilon\n",
    "min_epsilon = 0.01\n",
    "epsilon_decay_factor = 0.001\n",
    "\n",
    "num_episodes = 10\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training the Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, kernel_initializer='random_uniform', activation='relu', input_dim = 16),\n",
    "    tf.keras.layers.Dense(128, kernel_initializer='random_uniform',activation='relu'),\n",
    "\n",
    "    tf.keras.layers.Dense(4, kernel_initializer='random_uniform',activation='relu')\n",
    "#       tf.keras.layers.Dense(128, activation='relu', input_dim = 16),\n",
    "#       tf.keras.layers.Dense(4, activation='relu')\n",
    "    \n",
    "])\n",
    "opt=tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='mse',       # mean squared error\n",
    "              metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)\n",
    "loss,acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "num_episodes=1\n",
    "sleep(2)\n",
    "for i in range(num_episodes):\n",
    "    table.gettbl()\n",
    "    tablestate = table.tablestate()\n",
    "    reward_so_far = 0\n",
    "    while game.statepredict()=='going':\n",
    "        action, q_vals = np.array([np.argmax(model.predict(tablestate))]), model.predict(tablestate)\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action[0] = np.random.randint(4)\n",
    "            \n",
    "        mv(keys[action[0]] )\n",
    "        sleep(0.3)\n",
    "        table.gettbl()\n",
    "        next_tablestate=table.tablestate()\n",
    "        \n",
    "        if np.array_equal(next_tablestate,tablestate):\n",
    "            reward=-100\n",
    "        else:\n",
    "            reward = tablestate.sum()-next_tablestate.sum()\n",
    "        \n",
    "        if game.statepredict()=='lost':\n",
    "            reward=-1000\n",
    "\n",
    "        max_q=np.max(model.predict(tablestate))\n",
    "        target_q_vals = q_vals\n",
    "        target_q_vals[0, action[0]] = reward + gamma * max_q\n",
    "        \n",
    "        print(table.gettbl(),reward, keys[action[0]])\n",
    "        model.fit(tablestate, target_q_vals)\n",
    "        \n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        tablestate = next_tablestate\n",
    "        reward_so_far += reward\n",
    "        sleep(0.1)\n",
    "\n",
    "    # epsilon decay\n",
    "    game.statepredict(save=True, folder='./data/lost_games/')\n",
    "    keyboard.press(Key.f5)\n",
    "    sleep(0.02)\n",
    "    keyboard.release(Key.f5)\n",
    "    sleep(3)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * math.exp(-epsilon_decay_factor * i)\n",
    "\n",
    "    reward_list.append(reward_so_far)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints/\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "\n",
    "# Create checkpoint callback\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "#                                                  save_weights_only=True,\n",
    "#                                                  verbose=1)\n",
    "\n",
    "model.save_weights('./checkpoints/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### in case we have wrong gamestate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.statepredict(save=True)\n",
    "table.gettbl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.fit()\n",
    "game.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 2\n",
    "total_reward = []\n",
    "\n",
    "# model = create_model()\n",
    "# model.load_weights(latest)\n",
    "# loss, acc = model.evaluate(test_images, test_labels)\n",
    " \n",
    "sleep(2)\n",
    "table.gettbl()\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    \n",
    "    while game.statepredict()=='going':\n",
    "        table.gettbl()\n",
    "        tablestate = table.tablestate()\n",
    "#         action = sess.run(best_action, feed_dict={input_obs: state})\n",
    "        action= np.array([np.argmax(model.predict(tablestate))])\n",
    "        mv(keys[action[0]])\n",
    "        sleep(2)\n",
    "        \n",
    "        if np.array_equal(next_tablestate,tablestate):\n",
    "            reward=-100\n",
    "        else:\n",
    "            reward = tablestate.sum()-next_tablestate.sum()\n",
    "        \n",
    "        if game.statepredict()=='lost':\n",
    "            reward=-1000\n",
    "            \n",
    "       # state, reward, done, _ = env.step(action[0])\n",
    "        total_reward.append(reward)\n",
    "\n",
    "        sleep(0.1)\n",
    "\n",
    "\n",
    "print(\"Average reward per episode: {}\".format(total_reward/float(num_episodes)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
